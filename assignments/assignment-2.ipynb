{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Retrieval-Augmented Question Answering with LLMs\n",
    "\n",
    "In this assignment, we will use a large language model (LLM) in a retrieval-augmented setup to answer questions from the Natural Questions dataset. We will evaluate the performance of the LLM using different prompting strategies and compare the results. The steps involved are as follows:\n",
    "\n",
    "1. **Evaluate an LLM on Natural Questions without context.**\n",
    "2. **Evaluate an LLM on Natural Questions with provided context.**\n",
    "3. **Set up a retriever to find relevant passages.**\n",
    "4. **Evaluate the LLM using retrieved context instead of provided context.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what purpose did seasonal monsoon winds have o...</td>\n",
       "      <td>enabled European empire expansion into the Ame...</td>\n",
       "      <td>The westerlies (blue arrows) and trade winds (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who got the first nobel prize in physics</td>\n",
       "      <td>Wilhelm Conrad Röntgen, of Germany</td>\n",
       "      <td>The award is presented in Stockholm at an annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when is the next deadpool movie being released</td>\n",
       "      <td>May 18, 2018</td>\n",
       "      <td>Though the original creative team of Reynolds,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where did the idea of fortnite come from</td>\n",
       "      <td>as a cross between Minecraft and Left 4 Dead</td>\n",
       "      <td>Fortnite is set in contemporary Earth, where t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which mode is used for short wave broadcast se...</td>\n",
       "      <td>MFSK Olivia</td>\n",
       "      <td>All one needs is a pair of transceivers, each ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what purpose did seasonal monsoon winds have o...   \n",
       "1           who got the first nobel prize in physics   \n",
       "2     when is the next deadpool movie being released   \n",
       "3           where did the idea of fortnite come from   \n",
       "4  which mode is used for short wave broadcast se...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  enabled European empire expansion into the Ame...   \n",
       "1                 Wilhelm Conrad Röntgen, of Germany   \n",
       "2                                       May 18, 2018   \n",
       "3       as a cross between Minecraft and Left 4 Dead   \n",
       "4                                        MFSK Olivia   \n",
       "\n",
       "                                        gold_context  \n",
       "0  The westerlies (blue arrows) and trade winds (...  \n",
       "1  The award is presented in Stockholm at an annu...  \n",
       "2  Though the original creative team of Reynolds,...  \n",
       "3  Fortnite is set in contemporary Earth, where t...  \n",
       "4  All one needs is a pair of transceivers, each ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "nq_data = pd.read_csv('nq_simplified.val.tsv', sep='\\t', header=None, names=['question', 'answer', 'gold_context'], quoting=3)\n",
    "\n",
    "nq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge1(gold, predicted):\n",
    "    assert(len(gold) == len(predicted))\n",
    "    n_p = 0\n",
    "    n_g = 0\n",
    "    n_c = 0\n",
    "    for g, p in zip(gold, predicted):\n",
    "        g = set(cleanup(g).strip().split())\n",
    "        p = set(cleanup(p).strip().split())\n",
    "        n_g += len(g)\n",
    "        n_p += len(p)\n",
    "        n_c += len(p.intersection(g))\n",
    "    pr = n_c / n_p\n",
    "    re = n_c / n_g\n",
    "    if pr > 0 and re > 0:\n",
    "        f1 = 2*pr*re/(pr + re)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    return pr, re, f1\n",
    "\n",
    "def cleanup(text):\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Evaluating an LLM on Natural Questions\n",
    "\n",
    "First, we will find an LLM on huginface hub that is small and fast to fitt on our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the model and tokenizer\n",
    " \"mistralai/Mixtral-8x7B-Instruct-v0.1\" \"microsoft/Phi-3-mini-4k-instruct\" \"mtgv/MobileLLaMA-1.4B-Base\" \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \n",
    "\"TheBloke/Llama-2-7B-Chat-AWQ\"\n",
    "\n",
    "\"\"\"\n",
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a simple pipline for QA and use a small subset of the dataset to get the predicted answers and evaluate the model using ROUGE-1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Rouge-1 Precision</th>\n",
       "      <th>Rouge-1 Recall</th>\n",
       "      <th>Rouge-1 F1</th>\n",
       "      <th>Method</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>0.046677</td>\n",
       "      <td>0.234127</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] {} [/INST]</td>\n",
       "      <td>0.047983</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] {} [/INST] Answer:</td>\n",
       "      <td>0.052021</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] You are a question answering system....</td>\n",
       "      <td>0.054459</td>\n",
       "      <td>0.295635</td>\n",
       "      <td>0.091975</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;Question: {} [INST]Generate a short-form an...</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>0.204365</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  Rouge-1 Precision  \\\n",
       "0                                                 {}           0.046677   \n",
       "1                                  [INST] {} [/INST]           0.047983   \n",
       "2                          [INST] {} [/INST] Answer:           0.052021   \n",
       "3  <s>[INST] You are a question answering system....           0.054459   \n",
       "4  <s>Question: {} [INST]Generate a short-form an...           0.066970   \n",
       "\n",
       "   Rouge-1 Recall  Rouge-1 F1           Method  Sample Size  \n",
       "0        0.234127    0.077836  without context          100  \n",
       "1        0.273810    0.081657  without context          100  \n",
       "2        0.293651    0.088385  without context          100  \n",
       "3        0.295635    0.091975  without context          100  \n",
       "4        0.204365    0.100881  without context          100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 24s, sys: 265 ms, total: 10min 24s\n",
      "Wall time: 10min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, \n",
    "                       pad_token_id=tokenizer.eos_token_id, max_new_tokens=400,)\n",
    "\n",
    "# Define a function to get answers from the model\n",
    "def get_answers(questions, prompt, max_new_tokens):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        p = prompt.format(question) \n",
    "        response = qa_pipeline(p,max_new_tokens=max_new_tokens)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "# Use a small subset of the dataset for testing\n",
    "prompts = [ \n",
    "    \"{}\",\n",
    "    '[INST] {} [/INST]',\n",
    "    '[INST] {} [/INST] Answer:',\n",
    "    '<s>[INST] You are a question answering system. Please give a short answer to the following user question.[/INST] Question: {}',\n",
    "    '<s>Question: {} [INST]Generate a short-form answer of either one word or one sentence to the question.[/INST]',\n",
    "          ]\n",
    "subset = nq_data.sample(100)\n",
    "questions = subset['question'].tolist()\n",
    "gold_answers = subset['answer'].tolist()\n",
    "results=[]\n",
    "for prompt in prompts:\n",
    "    predicted_answers = get_answers(questions, prompt,max_new_tokens=50)\n",
    "    pr, re, f1 = rouge1(gold_answers, predicted_answers)\n",
    "    result = [prompt, pr, re, f1, \"without context\", 100]\n",
    "    results.append(result)\n",
    "result = pd.DataFrame(results, columns=['Prompt', 'Rouge-1 Precision', 'Rouge-1 Recall', 'Rouge-1 F1', 'Method', 'Sample Size'])\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: An Idealized Retrieval-Augmented LLM\n",
    "\n",
    "Now, we will include the `gold_context` in our prompts and evaluate the model again. This will help us understand the upper limits of the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 7s, sys: 4.54 s, total: 10min 12s\n",
      "Wall time: 10min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Rouge-1 Precision</th>\n",
       "      <th>Rouge-1 Recall</th>\n",
       "      <th>Rouge-1 F1</th>\n",
       "      <th>Method</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>0.046677</td>\n",
       "      <td>0.234127</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] {} [/INST]</td>\n",
       "      <td>0.047983</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] {} [/INST] Answer:</td>\n",
       "      <td>0.052021</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] You are a question answering system....</td>\n",
       "      <td>0.054459</td>\n",
       "      <td>0.295635</td>\n",
       "      <td>0.091975</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;Question: {} [INST]Generate a short-form an...</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>0.204365</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Context:{}, Question: {}, Answer:</td>\n",
       "      <td>0.130324</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.214586</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST]</td>\n",
       "      <td>0.114516</td>\n",
       "      <td>0.593254</td>\n",
       "      <td>0.191974</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST] Answer:</td>\n",
       "      <td>0.115204</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.192408</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n [INST] You are a question answ...</td>\n",
       "      <td>0.121236</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.202548</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n Question:{} [INST] Using the i...</td>\n",
       "      <td>0.178444</td>\n",
       "      <td>0.496032</td>\n",
       "      <td>0.262467</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  Rouge-1 Precision  \\\n",
       "0                                                 {}           0.046677   \n",
       "1                                  [INST] {} [/INST]           0.047983   \n",
       "2                          [INST] {} [/INST] Answer:           0.052021   \n",
       "3  <s>[INST] You are a question answering system....           0.054459   \n",
       "4  <s>Question: {} [INST]Generate a short-form an...           0.066970   \n",
       "5                  Context:{}, Question: {}, Answer:           0.130324   \n",
       "6             [INST] Context:{}\\n Question:{}[/INST]           0.114516   \n",
       "7     [INST] Context:{}\\n Question:{}[/INST] Answer:           0.115204   \n",
       "8  <s>Context:{}\\n [INST] You are a question answ...           0.121236   \n",
       "9  <s>Context:{}\\n Question:{} [INST] Using the i...           0.178444   \n",
       "\n",
       "   Rouge-1 Recall  Rouge-1 F1           Method  Sample Size  \n",
       "0        0.234127    0.077836  without context          100  \n",
       "1        0.273810    0.081657  without context          100  \n",
       "2        0.293651    0.088385  without context          100  \n",
       "3        0.295635    0.091975  without context          100  \n",
       "4        0.204365    0.100881  without context          100  \n",
       "5        0.607143    0.214586     with context          100  \n",
       "6        0.593254    0.191974     with context          100  \n",
       "7        0.583333    0.192408     with context          100  \n",
       "8        0.615079    0.202548     with context          100  \n",
       "9        0.496032    0.262467     with context          100  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Define a function to get answers from the model with context\n",
    "def get_answers_with_context(questions, contexts, prompt, max_new_tokens):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        p = prompt.format(context,question) \n",
    "        response = qa_pipeline(p, max_new_tokens=max_new_tokens)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "contexts = subset['gold_context'].tolist()\n",
    "\n",
    "prompts = [ \n",
    "    'Context:{}, Question: {}, Answer:',\n",
    "    '[INST] Context:{}\\n Question:{}[/INST]',\n",
    "    '[INST] Context:{}\\n Question:{}[/INST] Answer:',\n",
    "    '<s>Context:{}\\n [INST] You are a question answering system. Based on the above context, generate a short answer to the following question.[/INST]\\n Question: {}',\n",
    "    '<s>Context:{}\\n Question:{} [INST] Using the information provided in the context, generate a short-form answer of either one word or one sentence to the question. [/INST]',\n",
    "          ]\n",
    "results=[]\n",
    "for prompt in prompts:\n",
    "    predicted_answers_with_context = get_answers_with_context(questions, contexts, prompt, max_new_tokens=50)\n",
    "    pr_context, re_context, f1_context = rouge1(gold_answers, predicted_answers_with_context)\n",
    "    answer = [prompt, pr_context, re_context, f1_context, \"with context\", 100]\n",
    "    results.append(answer)\n",
    "result_2 = pd.DataFrame(results, columns=['Prompt', 'Rouge-1 Precision', 'Rouge-1 Recall', 'Rouge-1 F1', 'Method', 'Sample Size'])\n",
    "result = pd.concat([result, result_2], ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting Up the Retriever\n",
    "\n",
    "In this step, we will set up a retriever to find relevant passages. We will use the SentenceTransformers model to create embeddings for the passages and set up a FAISS index for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with open('passages.txt', 'r', encoding='utf-8') as file:\n",
    "    passages = file.readlines()\n",
    "\n",
    "embedded_passages = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "index = faiss.IndexFlatL2(embedded_passages.shape[1])\n",
    "index.add(embedded_passages.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Passage: Black Americans were not permitted to fly for the U.S. armed services prior to 1940. The Air Corps at that time, which had never had a single black member, was part of an army that possessed exactly two black Regular line officers at the beginning of World War II: Brigadier Generals Benjamin O. Davis, Sr. and Benjamin O. Davis, Jr. The first Civilian Pilot Training Program (CPTP) students completed their instruction in May 1940. The creation of an all-black pursuit squadron resulted from pressure by civil rights organizations and the black press who pushed for the establishment of a unit at Tuskegee, an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_best_passage(question):\n",
    "    question_embedding = retriever_model.encode([question], convert_to_tensor=True)\n",
    "    _, ix = index.search(question_embedding.cpu(), 1)\n",
    "    return passages[ix[0][0]]\n",
    "\n",
    "question = \"Where did the first African American air force unit train?\"\n",
    "best_passage = retrieve_best_passage(question)\n",
    "print(f\"Best Passage: {best_passage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Putting the Pieces Together\n",
    "\n",
    "Finally, we will retrieve passages for each question and evaluate the model using these passages instead of the provided `gold_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Rouge-1 Precision</th>\n",
       "      <th>Rouge-1 Recall</th>\n",
       "      <th>Rouge-1 F1</th>\n",
       "      <th>Method</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>0.046677</td>\n",
       "      <td>0.234127</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] {} [/INST]</td>\n",
       "      <td>0.047983</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] {} [/INST] Answer:</td>\n",
       "      <td>0.052021</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] You are a question answering system....</td>\n",
       "      <td>0.054459</td>\n",
       "      <td>0.295635</td>\n",
       "      <td>0.091975</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;Question: {} [INST]Generate a short-form an...</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>0.204365</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>without context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Context:{}, Question: {}, Answer:</td>\n",
       "      <td>0.130324</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.214586</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST]</td>\n",
       "      <td>0.114516</td>\n",
       "      <td>0.593254</td>\n",
       "      <td>0.191974</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST] Answer:</td>\n",
       "      <td>0.115204</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.192408</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n [INST] You are a question answ...</td>\n",
       "      <td>0.121236</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.202548</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n Question:{} [INST] Using the i...</td>\n",
       "      <td>0.178444</td>\n",
       "      <td>0.496032</td>\n",
       "      <td>0.262467</td>\n",
       "      <td>with context</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Context:{}, Question: {}, Answer:</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST]</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[INST] Context:{}\\n Question:{}[/INST] Answer:</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n [INST] You are a question answ...</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;s&gt;Context:{}\\n Question:{} [INST] Using the i...</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;s&gt; Context: {}\\n Question: {} [INST] Select t...</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.265873</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>Retriever</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Prompt  Rouge-1 Precision  \\\n",
       "0                                                  {}           0.046677   \n",
       "1                                   [INST] {} [/INST]           0.047983   \n",
       "2                           [INST] {} [/INST] Answer:           0.052021   \n",
       "3   <s>[INST] You are a question answering system....           0.054459   \n",
       "4   <s>Question: {} [INST]Generate a short-form an...           0.066970   \n",
       "5                   Context:{}, Question: {}, Answer:           0.130324   \n",
       "6              [INST] Context:{}\\n Question:{}[/INST]           0.114516   \n",
       "7      [INST] Context:{}\\n Question:{}[/INST] Answer:           0.115204   \n",
       "8   <s>Context:{}\\n [INST] You are a question answ...           0.121236   \n",
       "9   <s>Context:{}\\n Question:{} [INST] Using the i...           0.178444   \n",
       "10                  Context:{}, Question: {}, Answer:           0.083802   \n",
       "11             [INST] Context:{}\\n Question:{}[/INST]           0.083802   \n",
       "12     [INST] Context:{}\\n Question:{}[/INST] Answer:           0.083802   \n",
       "13  <s>Context:{}\\n [INST] You are a question answ...           0.083802   \n",
       "14  <s>Context:{}\\n Question:{} [INST] Using the i...           0.083802   \n",
       "15  <s> Context: {}\\n Question: {} [INST] Select t...           0.083802   \n",
       "\n",
       "    Rouge-1 Recall  Rouge-1 F1           Method  Sample Size  \n",
       "0         0.234127    0.077836  without context          100  \n",
       "1         0.273810    0.081657  without context          100  \n",
       "2         0.293651    0.088385  without context          100  \n",
       "3         0.295635    0.091975  without context          100  \n",
       "4         0.204365    0.100881  without context          100  \n",
       "5         0.607143    0.214586     with context          100  \n",
       "6         0.593254    0.191974     with context          100  \n",
       "7         0.583333    0.192408     with context          100  \n",
       "8         0.615079    0.202548     with context          100  \n",
       "9         0.496032    0.262467     with context          100  \n",
       "10        0.265873    0.127437        Retriever          100  \n",
       "11        0.265873    0.127437        Retriever          100  \n",
       "12        0.265873    0.127437        Retriever          100  \n",
       "13        0.265873    0.127437        Retriever          100  \n",
       "14        0.265873    0.127437        Retriever          100  \n",
       "15        0.265873    0.127437        Retriever          100  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answers_with_retrieved_context(questions, prompt, max_new_tokens):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        context = retrieve_best_passage(question)\n",
    "        prompt = f\"<s> Context: {context}\\n Question: {question} [INST] Using the information provided in the context, generate a short-form answer of either one word or one sentence to the question. [/INST]\"\n",
    "        response = qa_pipeline(prompt, max_new_tokens=max_new_tokens)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "prompts = [ \n",
    "    'Context:{}, Question: {}, Answer:',\n",
    "    '[INST] Context:{}\\n Question:{}[/INST]',\n",
    "    '[INST] Context:{}\\n Question:{}[/INST] Answer:',\n",
    "    '<s>Context:{}\\n [INST] You are a question answering system. Based on the above context, generate a short answer to the following question.[/INST]\\n Question: {}',\n",
    "    '<s>Context:{}\\n Question:{} [INST] Using the information provided in the context, generate a short-form answer of either one word or one sentence to the question. [/INST]',\n",
    "    '<s> Context: {}\\n Question: {} [INST] Select the passage from the provided context that best answers the question in either one word or a sentence. [/INST]',\n",
    "          ]\n",
    "results=[]\n",
    "for prompt in prompts:\n",
    "    predicted_answers_retrieved_context = get_answers_with_retrieved_context(questions,prompt, max_new_tokens=50)\n",
    "    pr_retrieved, re_retrieved, f1_retrieved = rouge1(gold_answers, predicted_answers_retrieved_context)\n",
    "    answer = [prompt, pr_retrieved, re_retrieved, f1_retrieved, \"Retriever\", 100]\n",
    "    results.append(answer)\n",
    "result_3 = pd.DataFrame(results, columns=['Prompt', 'Rouge-1 Precision', 'Rouge-1 Recall', 'Rouge-1 F1', 'Method', 'Sample Size'])\n",
    "result = pd.concat([result, result_3], ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we evaluated the performance of an LLM on the Natural Questions dataset using different prompting strategies. We observed the following:\n",
    "\n",
    "1. **Without Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "2. **With Provided Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "3. **With Retrieved Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "Including context, whether provided or retrieved, ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
