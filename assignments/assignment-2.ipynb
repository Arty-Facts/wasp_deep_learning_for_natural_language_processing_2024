{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Retrieval-Augmented Question Answering with LLMs\n",
    "\n",
    "In this assignment, we will use a large language model (LLM) in a retrieval-augmented setup to answer questions from the Natural Questions dataset. We will evaluate the performance of the LLM using different prompting strategies and compare the results. The steps involved are as follows:\n",
    "\n",
    "1. **Evaluate an LLM on Natural Questions without context.**\n",
    "2. **Evaluate an LLM on Natural Questions with provided context.**\n",
    "3. **Set up a retriever to find relevant passages.**\n",
    "4. **Evaluate the LLM using retrieved context instead of provided context.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what purpose did seasonal monsoon winds have o...</td>\n",
       "      <td>enabled European empire expansion into the Ame...</td>\n",
       "      <td>The westerlies (blue arrows) and trade winds (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who got the first nobel prize in physics</td>\n",
       "      <td>Wilhelm Conrad Röntgen, of Germany</td>\n",
       "      <td>The award is presented in Stockholm at an annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when is the next deadpool movie being released</td>\n",
       "      <td>May 18, 2018</td>\n",
       "      <td>Though the original creative team of Reynolds,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where did the idea of fortnite come from</td>\n",
       "      <td>as a cross between Minecraft and Left 4 Dead</td>\n",
       "      <td>Fortnite is set in contemporary Earth, where t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which mode is used for short wave broadcast se...</td>\n",
       "      <td>MFSK Olivia</td>\n",
       "      <td>All one needs is a pair of transceivers, each ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what purpose did seasonal monsoon winds have o...   \n",
       "1           who got the first nobel prize in physics   \n",
       "2     when is the next deadpool movie being released   \n",
       "3           where did the idea of fortnite come from   \n",
       "4  which mode is used for short wave broadcast se...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  enabled European empire expansion into the Ame...   \n",
       "1                 Wilhelm Conrad Röntgen, of Germany   \n",
       "2                                       May 18, 2018   \n",
       "3       as a cross between Minecraft and Left 4 Dead   \n",
       "4                                        MFSK Olivia   \n",
       "\n",
       "                                        gold_context  \n",
       "0  The westerlies (blue arrows) and trade winds (...  \n",
       "1  The award is presented in Stockholm at an annu...  \n",
       "2  Though the original creative team of Reynolds,...  \n",
       "3  Fortnite is set in contemporary Earth, where t...  \n",
       "4  All one needs is a pair of transceivers, each ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "nq_data = pd.read_csv('nq_simplified.val.tsv', sep='\\t', header=None, names=['question', 'answer', 'gold_context'], quoting=3)\n",
    "\n",
    "nq_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge1(gold, predicted):\n",
    "    assert(len(gold) == len(predicted))\n",
    "    n_p = 0\n",
    "    n_g = 0\n",
    "    n_c = 0\n",
    "    for g, p in zip(gold, predicted):\n",
    "        g = set(cleanup(g).strip().split())\n",
    "        p = set(cleanup(p).strip().split())\n",
    "        n_g += len(g)\n",
    "        n_p += len(p)\n",
    "        n_c += len(p.intersection(g))\n",
    "    pr = n_c / n_p\n",
    "    re = n_c / n_g\n",
    "    if pr > 0 and re > 0:\n",
    "        f1 = 2*pr*re/(pr + re)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    return pr, re, f1\n",
    "\n",
    "def cleanup(text):\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Evaluating an LLM on Natural Questions\n",
    "\n",
    "First, we will find an LLM on huginface hub that is small and fast to fitt on our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638a27cceab14251bbe0fecb04b068de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "# \"mistralai/Mixtral-8x7B-Instruct-v0.1\" \"microsoft/Phi-3-mini-4k-instruct\" \"mtgv/MobileLLaMA-1.4B-Base\" \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a simple pipline for QA and use a small subset of the dataset to get the predicted answers and evaluate the model using ROUGE-1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.0, Recall: 0.0, F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a function to get answers from the model\n",
    "def get_answers(questions):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        response = qa_pipeline(question, max_new_tokens=64)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "# Use a small subset of the dataset for testing\n",
    "subset = nq_data.sample(1)\n",
    "questions = subset['question'].tolist()\n",
    "gold_answers = subset['answer'].tolist()\n",
    "\n",
    "predicted_answers = get_answers(questions)\n",
    "\n",
    "# Evaluate the model\n",
    "pr, re, f1 = rouge1(gold_answers, predicted_answers)\n",
    "print(f\"ROUGE-1 Precision: {pr}, Recall: {re}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: An Idealized Retrieval-Augmented LLM\n",
    "\n",
    "Now, we will include the `gold_context` in our prompts and evaluate the model again. This will help us understand the upper limits of the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Context - ROUGE-1 Precision: 0.03225806451612903, Recall: 1.0, F1 Score: 0.0625\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get answers from the model with context\n",
    "def get_answers_with_context(questions, contexts):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        response = qa_pipeline(prompt, max_new_tokens=64)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "contexts = subset['gold_context'].tolist()\n",
    "\n",
    "predicted_answers_with_context = get_answers_with_context(questions, contexts)\n",
    "\n",
    "pr_context, re_context, f1_context = rouge1(gold_answers, predicted_answers_with_context)\n",
    "print(f\"With Context - ROUGE-1 Precision: {pr_context}, Recall: {re_context}, F1 Score: {f1_context}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting Up the Retriever\n",
    "\n",
    "In this step, we will set up a retriever to find relevant passages. We will use the SentenceTransformers model to create embeddings for the passages and set up a FAISS index for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl (14.5 MB)\n",
      "     --------------------------------------- 14.5/14.5 MB 14.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in p:\\wasp_deep_learning_for_natural_language_processing_2024\\venv\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'P:\\wasp_deep_learning_for_natural_language_processing_2024\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss-cpu #windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with open('passages.txt', 'r', encoding='utf-8') as file:\n",
    "    passages = file.readlines()\n",
    "\n",
    "embedded_passages = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "index = faiss.IndexFlatL2(embedded_passages.shape[1])\n",
    "index.add(embedded_passages.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Passage: Black Americans were not permitted to fly for the U.S. armed services prior to 1940. The Air Corps at that time, which had never had a single black member, was part of an army that possessed exactly two black Regular line officers at the beginning of World War II: Brigadier Generals Benjamin O. Davis, Sr. and Benjamin O. Davis, Jr. The first Civilian Pilot Training Program (CPTP) students completed their instruction in May 1940. The creation of an all-black pursuit squadron resulted from pressure by civil rights organizations and the black press who pushed for the establishment of a unit at Tuskegee, an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_best_passage(question):\n",
    "    question_embedding = retriever_model.encode([question], convert_to_tensor=True)\n",
    "    _, ix = index.search(question_embedding.cpu(), 1)\n",
    "    return passages[ix[0][0]]\n",
    "\n",
    "question = \"Where did the first African American air force unit train?\"\n",
    "best_passage = retrieve_best_passage(question)\n",
    "print(f\"Best Passage: {best_passage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Putting the Pieces Together\n",
    "\n",
    "Finally, we will retrieve passages for each question and evaluate the model using these passages instead of the provided `gold_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Retrieved Context - ROUGE-1 Precision: 0.02127659574468085, Recall: 1.0, F1 Score: 0.04166666666666667\n"
     ]
    }
   ],
   "source": [
    "def get_answers_with_retrieved_context(questions):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        context = retrieve_best_passage(question)\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        response = qa_pipeline(prompt, max_new_tokens=64)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "predicted_answers_retrieved_context = get_answers_with_retrieved_context(questions)\n",
    "\n",
    "pr_retrieved, re_retrieved, f1_retrieved = rouge1(gold_answers, predicted_answers_retrieved_context)\n",
    "print(f\"With Retrieved Context - ROUGE-1 Precision: {pr_retrieved}, Recall: {re_retrieved}, F1 Score: {f1_retrieved}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we evaluated the performance of an LLM on the Natural Questions dataset using different prompting strategies. We observed the following:\n",
    "\n",
    "1. **Without Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "2. **With Provided Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "3. **With Retrieved Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "Including context, whether provided or retrieved, ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
