{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Retrieval-Augmented Question Answering with LLMs\n",
    "\n",
    "In this assignment, we will use a large language model (LLM) in a retrieval-augmented setup to answer questions from the Natural Questions dataset. We will evaluate the performance of the LLM using different prompting strategies and compare the results. The steps involved are as follows:\n",
    "\n",
    "1. **Evaluate an LLM on Natural Questions without context.**\n",
    "2. **Evaluate an LLM on Natural Questions with provided context.**\n",
    "3. **Set up a retriever to find relevant passages.**\n",
    "4. **Evaluate the LLM using retrieved context instead of provided context.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what purpose did seasonal monsoon winds have o...</td>\n",
       "      <td>enabled European empire expansion into the Ame...</td>\n",
       "      <td>The westerlies (blue arrows) and trade winds (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who got the first nobel prize in physics</td>\n",
       "      <td>Wilhelm Conrad Röntgen, of Germany</td>\n",
       "      <td>The award is presented in Stockholm at an annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when is the next deadpool movie being released</td>\n",
       "      <td>May 18, 2018</td>\n",
       "      <td>Though the original creative team of Reynolds,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where did the idea of fortnite come from</td>\n",
       "      <td>as a cross between Minecraft and Left 4 Dead</td>\n",
       "      <td>Fortnite is set in contemporary Earth, where t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which mode is used for short wave broadcast se...</td>\n",
       "      <td>MFSK Olivia</td>\n",
       "      <td>All one needs is a pair of transceivers, each ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what purpose did seasonal monsoon winds have o...   \n",
       "1           who got the first nobel prize in physics   \n",
       "2     when is the next deadpool movie being released   \n",
       "3           where did the idea of fortnite come from   \n",
       "4  which mode is used for short wave broadcast se...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  enabled European empire expansion into the Ame...   \n",
       "1                 Wilhelm Conrad Röntgen, of Germany   \n",
       "2                                       May 18, 2018   \n",
       "3       as a cross between Minecraft and Left 4 Dead   \n",
       "4                                        MFSK Olivia   \n",
       "\n",
       "                                        gold_context  \n",
       "0  The westerlies (blue arrows) and trade winds (...  \n",
       "1  The award is presented in Stockholm at an annu...  \n",
       "2  Though the original creative team of Reynolds,...  \n",
       "3  Fortnite is set in contemporary Earth, where t...  \n",
       "4  All one needs is a pair of transceivers, each ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "nq_data = pd.read_csv('nq_simplified.val.tsv', sep='\\t', header=None, names=['question', 'answer', 'gold_context'], quoting=3)\n",
    "\n",
    "nq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 17 13:24:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:21:00.0  On |                  N/A |\n",
      "|  0%   36C    P8              11W / 320W |   1259MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2620      G   /usr/lib/xorg/Xorg                          610MiB |\n",
      "|    0   N/A  N/A      2771      G   /usr/bin/gnome-shell                         26MiB |\n",
      "|    0   N/A  N/A      3318      G   ...AAAAAAAACAAAAAAAAAA= --shared-files      105MiB |\n",
      "|    0   N/A  N/A      4833      G   ...seed-version=20240429-180218.438000      277MiB |\n",
      "|    0   N/A  N/A      5573      G   ...sion,SpareRendererForSitePerProcess      197MiB |\n",
      "|    0   N/A  N/A      6052      G   ...yOnDemand --variations-seed-version       25MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge1(gold, predicted):\n",
    "    assert(len(gold) == len(predicted))\n",
    "    n_p = 0\n",
    "    n_g = 0\n",
    "    n_c = 0\n",
    "    for g, p in zip(gold, predicted):\n",
    "        g = set(cleanup(g).strip().split())\n",
    "        p = set(cleanup(p).strip().split())\n",
    "        n_g += len(g)\n",
    "        n_p += len(p)\n",
    "        n_c += len(p.intersection(g))\n",
    "    pr = n_c / n_p\n",
    "    re = n_c / n_g\n",
    "    if pr > 0 and re > 0:\n",
    "        f1 = 2*pr*re/(pr + re)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    return pr, re, f1\n",
    "\n",
    "def cleanup(text):\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Evaluating an LLM on Natural Questions\n",
    "\n",
    "First, we will find an LLM on huginface hub that is small and fast to fitt on our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148541e36c2249f69dba4a3ae6380cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9141131d8a2b4148a1264ebedd5cd672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84840168d4b84aa89d11f52958d06259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b046dfff86ac44f4b56aecc5506cebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3450f9941ce8481895c38c34d8a92362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6254e95171dc40f08f183829011bdbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f011e59a80d642fba9abe6170e6e1520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the model and tokenizer\n",
    " \"mistralai/Mixtral-8x7B-Instruct-v0.1\" \"microsoft/Phi-3-mini-4k-instruct\" \"mtgv/MobileLLaMA-1.4B-Base\" \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \n",
    "\"TheBloke/Llama-2-7B-Chat-AWQ\"\n",
    "\n",
    "\"\"\"\n",
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a simple pipline for QA and use a small subset of the dataset to get the predicted answers and evaluate the model using ROUGE-1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.013316423589093214, Recall: 0.3333333333333333, F1 Score: 0.025609756097560974\n",
      "CPU times: user 2min 51s, sys: 20 ms, total: 2min 51s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, \n",
    "                       pad_token_id=tokenizer.eos_token_id, max_new_tokens=400,)\n",
    "\n",
    "# Define a function to get answers from the model\n",
    "def get_answers(questions, max_new_tokens=400):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        response = qa_pipeline(question)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "# Use a small subset of the dataset for testing\n",
    "subset = nq_data.sample(20)\n",
    "questions = subset['question'].tolist()\n",
    "gold_answers = subset['answer'].tolist()\n",
    "\n",
    "predicted_answers = get_answers(questions)\n",
    "\n",
    "# Evaluate the model\n",
    "pr, re, f1 = rouge1(gold_answers, predicted_answers)\n",
    "print(f\"ROUGE-1 Precision: {pr}, Recall: {re}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: An Idealized Retrieval-Augmented LLM\n",
    "\n",
    "Now, we will include the `gold_context` in our prompts and evaluate the model again. This will help us understand the upper limits of the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Context - ROUGE-1 Precision: 0.08614864864864864, Recall: 0.8095238095238095, F1 Score: 0.15572519083969466\n",
      "CPU times: user 36 s, sys: 0 ns, total: 36 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define a function to get answers from the model with context\n",
    "def get_answers_with_context(questions, contexts):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        response = qa_pipeline(prompt)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "contexts = subset['gold_context'].tolist()\n",
    "\n",
    "predicted_answers_with_context = get_answers_with_context(questions, contexts)\n",
    "\n",
    "pr_context, re_context, f1_context = rouge1(gold_answers, predicted_answers_with_context)\n",
    "print(f\"With Context - ROUGE-1 Precision: {pr_context}, Recall: {re_context}, F1 Score: {f1_context}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting Up the Retriever\n",
    "\n",
    "In this step, we will set up a retriever to find relevant passages. We will use the SentenceTransformers model to create embeddings for the passages and set up a FAISS index for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with open('passages.txt', 'r', encoding='utf-8') as file:\n",
    "    passages = file.readlines()\n",
    "\n",
    "embedded_passages = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "index = faiss.IndexFlatL2(embedded_passages.shape[1])\n",
    "index.add(embedded_passages.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Passage: Black Americans were not permitted to fly for the U.S. armed services prior to 1940. The Air Corps at that time, which had never had a single black member, was part of an army that possessed exactly two black Regular line officers at the beginning of World War II: Brigadier Generals Benjamin O. Davis, Sr. and Benjamin O. Davis, Jr. The first Civilian Pilot Training Program (CPTP) students completed their instruction in May 1940. The creation of an all-black pursuit squadron resulted from pressure by civil rights organizations and the black press who pushed for the establishment of a unit at Tuskegee, an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_best_passage(question):\n",
    "    question_embedding = retriever_model.encode([question], convert_to_tensor=True)\n",
    "    _, ix = index.search(question_embedding.cpu(), 1)\n",
    "    return passages[ix[0][0]]\n",
    "\n",
    "question = \"Where did the first African American air force unit train?\"\n",
    "best_passage = retrieve_best_passage(question)\n",
    "print(f\"Best Passage: {best_passage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Putting the Pieces Together\n",
    "\n",
    "Finally, we will retrieve passages for each question and evaluate the model using these passages instead of the provided `gold_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Retrieved Context - ROUGE-1 Precision: 0.030193236714975844, Recall: 0.3968253968253968, F1 Score: 0.056116722783389444\n"
     ]
    }
   ],
   "source": [
    "def get_answers_with_retrieved_context(questions):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        context = retrieve_best_passage(question)\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        response = qa_pipeline(prompt)\n",
    "        answers.append(response[0]['generated_text'])\n",
    "    return answers\n",
    "\n",
    "predicted_answers_retrieved_context = get_answers_with_retrieved_context(questions)\n",
    "\n",
    "pr_retrieved, re_retrieved, f1_retrieved = rouge1(gold_answers, predicted_answers_retrieved_context)\n",
    "print(f\"With Retrieved Context - ROUGE-1 Precision: {pr_retrieved}, Recall: {re_retrieved}, F1 Score: {f1_retrieved}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we evaluated the performance of an LLM on the Natural Questions dataset using different prompting strategies. We observed the following:\n",
    "\n",
    "1. **Without Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "2. **With Provided Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "3. **With Retrieved Context**:\n",
    "   - ROUGE-1 Precision: *result*\n",
    "   - Recall: *result*\n",
    "   - F1 Score: *result*\n",
    "\n",
    "Including context, whether provided or retrieved, ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
