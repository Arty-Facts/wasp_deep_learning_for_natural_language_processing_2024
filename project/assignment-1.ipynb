{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement and train two or three neural language models: the fixed-window model, the recurrent neural network model from Unit&nbsp;1-2, and optionally a model based on the Transformer architecture from Unit&nbsp;1-3. You will evaluate these models by computing their perplexity on a benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you should use the GPU if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # NVIDIA\n",
    "# device = torch.device('mps')    # Apple Silicon\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this assignment is [WikiText](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100 million tokens extracted from the ‚ÄúGood‚Äù and ‚ÄúFeatured‚Äù articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\n",
    "\n",
    "The next cell contains code for an object that will act as a container for the ‚Äútraining‚Äù and the ‚Äúvalidation‚Äù section of the data. We fill this container by reading the corresponding text files. The only processing we do is to whitespace-tokenise and to replace each newline with an end-of-sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "\n",
    "class WikiText(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.train = self.read_data('wiki.train.tokens')\n",
    "        self.valid = self.read_data('wiki.valid.tokens')\n",
    "\n",
    "    def read_data(self, path):\n",
    "        ids = []\n",
    "        with open(path, encoding='utf-8') as source:\n",
    "            for line in source:\n",
    "                for word in line.split() + ['<eos>']:\n",
    "                    if word not in self.word2idx:\n",
    "                        self.word2idx[word] = len(self.word2idx)\n",
    "                        self.idx2word.append(word)\n",
    "                    ids.append(self.word2idx[word])\n",
    "        return ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the data and prints the total number of tokens and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in train: 2088628\n",
      "Tokens in valid: 217646\n",
      "Vocabulary size: 33278\n"
     ]
    }
   ],
   "source": [
    "wikitext = WikiText()\n",
    "UNK_IDX = wikitext.word2idx[UNK]\n",
    "\n",
    "print('Tokens in train:', len(wikitext.train))\n",
    "print('Tokens in valid:', len(wikitext.valid))\n",
    "print('Vocabulary size:', len(wikitext.word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement and train the fixed-window neural language model proposed by [Bengio et al. (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and presented in the lectures. Recall that an input to the network takes the form of a vector of $n-1$ integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1: Vectorise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement a [collate function](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) in the form of a callable vectoriser object. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowVectorizer(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(data) - self.n + 1):\n",
    "            ngram = data[i:i+self.n]\n",
    "            X.append(ngram[:-1])\n",
    "            y.append(ngram[-1])\n",
    "        return torch.tensor(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should implement the following specification:\n",
    "\n",
    "**__init__** (*self*, *n*)\n",
    "\n",
    "> Creates a new vectoriser with n-gram order $n$. Your code should be able to handle arbitrary n-gram orders $n \\geq 1$.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms WikiText *data* (a list of word ids) into a pair of tensors $\\mathbf{X}$, $\\mathbf{y}$ that can be used to train the fixed-window model. Let $N$ be the total number of $n$-grams from the token list; then $\\mathbf{X}$ is a matrix with shape $(N, n-1)$ and $\\mathbf{y}$ is a vector with length $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the code in the next cell. Does the output match your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217644, 2]) torch.Size([217644])\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = FixedWindowVectorizer(3)(wikitext.valid)\n",
    "\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2: Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the fixed-window model based on the graphical specification given in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FixedWindowModel(nn.Module):\n",
    "    def __init__(self, n, n_words, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embedding_dim)\n",
    "        self.n = n-1\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * self.n, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_words)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, *_ = x.shape\n",
    "        x = self.embedding(x).view(batch, -1)\n",
    "        return self.head(x)\n",
    "    \n",
    "    def generate(self, context, n=10, temperature=1.0):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        if len(context) < self.n:\n",
    "            context = torch.cat([torch.full((self.n - len(context),), UNK_IDX, dtype=torch.long), context])\n",
    "        with torch.no_grad():\n",
    "            context = context[-self.n:].to(device)\n",
    "            for _ in range(n):\n",
    "                logits = self(context.unsqueeze(0)).squeeze(0)\n",
    "                word = torch.multinomial(torch.softmax(logits / temperature, dim=-1), 1)\n",
    "                context = torch.cat([context[1:], word])\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the two methods:\n",
    "\n",
    "**__init__** (*self*, *n*, *n_words*, *embedding_dim*=64, *hidden_dim*=64)\n",
    "\n",
    "> Creates a new fixed-window neural language model. The argument *n* specifies the model&rsquo;s $n$-gram order. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 64.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, n-1)$, where $B$ is the batch size. The output of the forward pass is a tensor of shape $(B, V)$ where $V$ is the number of words in the vocabulary.\n",
    "\n",
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops that you have seen so far. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def train_fixed_window(n, n_epochs=2, batch_size=3072, lr=1e-2):\n",
    "    model = FixedWindowModel(n, len(wikitext))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_x, train_y = FixedWindowVectorizer(n)(wikitext.train)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    valid_x, valid_y = FixedWindowVectorizer(n)(wikitext.valid)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    perplexitys = []\n",
    "\n",
    "    with tqdm.tqdm(total = n_epochs) as pbar:\n",
    "      for t in range(n_epochs):\n",
    "        pbar.set_description(f'Epoch {t+1}')\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "          x, y = x.to(device), y.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          output = model(x)\n",
    "          loss = F.cross_entropy(output, y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss += loss.item()\n",
    "        train_loss=train_loss/len(train_loader)\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output_valid = model(x)\n",
    "                valid_loss += F.cross_entropy(output_valid, y).cpu().item()\n",
    "              \n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        perplexity = torch.exp(torch.tensor(valid_loss))\n",
    "        pbar.set_postfix(perplexity=perplexity.item(), train_loss=train_loss, valid_loss=valid_loss)\n",
    "        pbar.update()\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        perplexitys.append(perplexity)\n",
    "\n",
    "    history = {'train_loss': train_losses, 'valid_loss': valid_losses, 'perplexity': perplexitys}\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*n*, *n_epochs* = 2, *batch_size* = 3072, *lr* = 0.01)\n",
    "\n",
    "> Trains a fixed-window neural language model of order *n* using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below trains a trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:43<00:00, 21.68s/it, perplexity=306, train_loss=5.5, valid_loss=5.72] \n"
     ]
    }
   ],
   "source": [
    "model_fixed_window, history = train_fixed_window(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "Your submitted notebook must contain output demonstrating a validation perplexity of **at most 360** after training for two epochs with the default parameters.\n",
    "\n",
    "‚ö†Ô∏è Computing the validation perplexity in one go (for the full validation set) will most probably exhaust your computer‚Äôs memory and/or take a lot of time. Instead, do the computation at the minibatch level and aggregate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To see whether your network is learning something, print or plot the loss and/or the perplexity on the training data. If the two values do not decrease during training, try to find the problem before wasting time (and electricity) on useless computation.\n",
    "\n",
    "Training and even evaluation will take some time ‚Äì on a CPU, you should expect several minutes per epoch, depending on hardware. Our reference implementation uses a GPU and runs in 45&nbsp;seconds on a MacBook Pro (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Recurrent neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position $i$ in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer representing the probability distribution over the words at position $i+1$. In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed value *bptt_len*. This length is called the **backpropagation-through-time horizon**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Vectorise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVectorizer(object):\n",
    "    def __init__(self, bptt_len):\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "    def __call__(self, data):\n",
    "        X ,Y = [], []\n",
    "        for idx in range(self.bptt_len, len(data) - 1):\n",
    "            X.append(data[idx - self.bptt_len:idx])\n",
    "            Y.append(data[idx - self.bptt_len + 1:idx + 1])\n",
    "        return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your vectoriser should meet the following specification:\n",
    "\n",
    "**__init__** (*self*, *bptt_len*)\n",
    "\n",
    "> Creates a new vectoriser. The parameter *bptt_len* specifies the backpropagation-through-time horizon.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms a list of token indexes *data* into a pair of tensors $\\mathbf{X}$, $\\mathbf{Y}$ that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length *bptt_len*. Compared to the sequences in $\\mathbf{X}$, the corresponding sequences in $\\mathbf{Y}$ are shifted one position to the right. More precisely, if the $i$th row of $\\mathbf{X}$ is the sequence that starts at token position $j$, then the same row of $\\mathbf{Y}$ is the sequence that starts at position $j+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217613, 32]) torch.Size([217613, 32])\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = RNNVectorizer(32)(wikitext.valid)\n",
    "\n",
    "print(valid_x.size(), valid_y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2: Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the recurrent neural network model based on the graphical specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_dim, n_words)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        if hidden is not None:\n",
    "            x, hidden = self.lstm(x, hidden)\n",
    "        else:\n",
    "            x, hidden = self.lstm(x)\n",
    "        return self.head(x), hidden\n",
    "    \n",
    "    def generate(self, context, n=10, temperature=1.0):\n",
    "        device = next(self.parameters()).device\n",
    "        hidden = None\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # build hidden state\n",
    "            for x in context:\n",
    "                x = x.unsqueeze(0).to(device)\n",
    "                _, hidden = self(x, hidden)\n",
    "            # generate next words\n",
    "            for _ in range(n):\n",
    "                x, hidden = self(x, hidden)\n",
    "                x = x / temperature\n",
    "                x = torch.multinomial(torch.softmax(x, dim=-1), 1)\n",
    "                x = x.squeeze(0)\n",
    "                yield x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should follow this specification:\n",
    "\n",
    "**__init__** (*self*, *n_words*, *embedding_dim* = 64, *hidden_dim* = 64)\n",
    "\n",
    "> Creates a new recurrent neural network language model based on an LSTM. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 64.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, H)$, where $B$ is the batch size and $H$ is the length of each input sequence. The shape of the output tensor is $(B, H, V)$, where $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(n_epochs=2, batch_size=3072, bptt_len=32, lr=1e-2):\n",
    "    model = RNNModel(len(wikitext))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_x, train_y = RNNVectorizer(bptt_len)(wikitext.train)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    valid_x, valid_y = RNNVectorizer(bptt_len)(wikitext.valid)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    perplexitys = []\n",
    "\n",
    "    with tqdm.tqdm(total = n_epochs) as pbar:\n",
    "      for t in range(n_epochs):\n",
    "        pbar.set_description(f'Epoch {t+1}')\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "          x, y = x.to(device), y.to(device)\n",
    "          output, _ = model(x)\n",
    "          # https://discuss.pytorch.org/t/pytorch-lstm-target-dimension-in-calculating-cross-entropy-loss/30398/10\n",
    "          B, C, _ = output.shape\n",
    "          output = output.view(B*C, -1)\n",
    "          y = y.view(B*C,)\n",
    "          loss = F.cross_entropy(output, y)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss += loss.item()\n",
    "        train_loss=train_loss/len(train_loader)\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        valid_losse = 0\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output_valid, _ = model(x)\n",
    "                B, C, _ = output_valid.shape\n",
    "                output_valid = output_valid.view(B*C, -1)\n",
    "                y = y.view(B*C,)\n",
    "                valid_losse += F.cross_entropy(output_valid, y).cpu().item()\n",
    "              \n",
    "        valid_losse = valid_losse / len(valid_loader)\n",
    "        perplexity = torch.exp(torch.tensor(valid_losse))\n",
    "        pbar.set_postfix(perplexity=perplexity.item(), train_loss=train_loss, valid_loss=valid_losse)\n",
    "        pbar.update()\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_losse)\n",
    "        perplexitys.append(perplexity)\n",
    "\n",
    "    history = {'train_loss': train_losses, 'valid_loss': valid_losses, 'perplexity': perplexitys}\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_rnn** (*n_epochs* = 2, *batch_size* = 3072, *bptt_len* = 32, *lr* = 0.01)\n",
    "\n",
    "> Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. The parameter *bptt_len* specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [08:15<00:00, 247.79s/it, perplexity=253, train_loss=5.01, valid_loss=5.53]\n"
     ]
    }
   ],
   "source": [
    "model_rnn, history = train_rnn(batch_size=3074//5, lr=1e-2/5) # if we drop the batch size, we need to drop the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "Your submitted notebook must contain output demonstrating a validation perplexity of **at most 280** after training for two epochs with the default hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Transformer model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are up for a challenge, try implementing a Transformer-based language model. The required vectoriser is identical to the vectoriser for the RNN model. For the model itself, you can use the Pytorch modules [`nn.TransformerEncoder`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) and [`nn.TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html).  To represent positional information, follow the approach from the original Transformer paper and use sine and cosine functions of different frequencies ([details](https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding)), or learn position-specific embeddings. Can you get a lower perplexity than for the RNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer model is defined based on the pytorch tutorial on Language Modeling with nn.Transformer and torchtext.\n",
    "URL: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "class TransformerVectorizer(object):\n",
    "    def __init__(self, seq_length ):\n",
    "        self.seq_length  = seq_length \n",
    "\n",
    "    def __call__(self, data):\n",
    "        X ,Y = [], []\n",
    "        for idx in range(self.seq_length, len(data) - 1):\n",
    "            X.append(data[idx - self.seq_length :idx])\n",
    "            Y.append(data[idx - self.seq_length  + 1:idx + 1])\n",
    "        return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,n_words, num_heads, num_layers, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words,embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        encoderlayer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoderlayer,num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, n_words)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        if x_mask is None:\n",
    "            x_mask = nn.Transformer.generate_square_subsequent_mask(len(x)).to(device)\n",
    "        x = self.transformer_encoder(x,x_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,embedding_dim, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def train_transformer(n_epochs=2, batch_size=3072, seq_length=32, lr=1e-2, init_func=None):\n",
    "    model = TransformerModel(len(wikitext), num_heads=2, num_layers=2)\n",
    "    if init_func is not None:\n",
    "        init_func(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_x, train_y = TransformerVectorizer(seq_length)(wikitext.train)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    valid_x, valid_y = TransformerVectorizer(seq_length)(wikitext.valid)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    perplexitys = []\n",
    "\n",
    "    with tqdm.tqdm(total = n_epochs) as pbar:\n",
    "      for t in range(n_epochs):\n",
    "        pbar.set_description(f'Epoch {t+1}')\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "          x, y = x.to(device), y.to(device)\n",
    "          output = model(x)\n",
    "          # https://discuss.pytorch.org/t/pytorch-lstm-target-dimension-in-calculating-cross-entropy-loss/30398/10\n",
    "          B, C, _ = output.shape\n",
    "          output = output.view(B*C, -1)\n",
    "          y = y.view(B*C,)\n",
    "          loss = F.cross_entropy(output, y)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss += loss.item()\n",
    "        train_loss=train_loss/len(train_loader)\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        valid_losse = 0\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output_valid = model(x)\n",
    "                B, C, _ = output_valid.shape\n",
    "                output_valid = output_valid.view(B*C, -1)\n",
    "                y = y.view(B*C,)\n",
    "                valid_losse += F.cross_entropy(output_valid, y).cpu().item()\n",
    "              \n",
    "        valid_losse = valid_losse / len(valid_loader)\n",
    "        perplexity = torch.exp(torch.tensor(valid_losse))\n",
    "        pbar.set_postfix(perplexity=perplexity.item(), train_loss=train_loss, valid_loss=valid_losse)\n",
    "        pbar.update()\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_losse)\n",
    "        perplexitys.append(perplexity)\n",
    "\n",
    "    history = {'train_loss': train_losses, 'valid_loss': valid_losses, 'perplexity': perplexitys}\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [08:37<00:00, 258.63s/it, perplexity=335, train_loss=5.07, valid_loss=5.81]\n"
     ]
    }
   ],
   "source": [
    "# Running on a smaller batch due to GPU memory issue \n",
    "model_transformer, history = train_transformer(batch_size = 3072//5, lr = 1e-2/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a simple generation mechanism for the language models you have implemented.\n",
    "\n",
    "Recall that one way to generate text with a language model is to repeatedly sample from the model‚Äôs output distribution, conditioning on some context. More specifically, this involves treating the softmax-normalised logits of the model as a multinomial distribution. The ‚Äúcreativeness‚Äù of the generation can be controlled with the temperature parameter of the softmax distribution.\n",
    "\n",
    "To implement this recipe, we first ask you to extend each model with a `generate` method according to the following specification:\n",
    "\n",
    "**generate** (*self*, *context*, *n_tokens* = 10, *temperature* = 1.0)\n",
    "\n",
    "> Takes a batch of context tokens *context* and extends it by sampling *n_tokens* new tokens from the model‚Äôs output distribution, scaled with the temperature *temperature*. Returns the extended context.\n",
    "\n",
    "In a second stage, you should implement a convenience function `generate` that allows you to easily generate text with different models, like this:\n",
    "\n",
    "```\n",
    "generate(model_fixed_window, 'i like', max_tokens=10, temperature=1.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nithesh/Project/wasp_deep_learning_for_natural_language_processing_2024/venv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        return [t.text.lower() for t in nlp.tokenizer(text)]\n",
    "    else:\n",
    "        return [t.text for t in nlp.tokenizer(text)]\n",
    "\n",
    "def generate(model, text, max_tokens=3, temperature=1.0):\n",
    "    context = torch.tensor([wikitext.word2idx.get(token, UNK_IDX) for token in tokenize(text)], dtype=torch.long)\n",
    "    generated_tokens = []\n",
    "    for token in model.generate(context, max_tokens, temperature):\n",
    "        word = wikitext.idx2word[token]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        generated_tokens.append(word)\n",
    "    return text+' '+' '.join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like God of War . Galveston were looking to survive ,'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model_fixed_window, 'i like', max_tokens=10, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i like <unk> 's work that do possibly elaborate starlings .\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model_rnn, 'i like', max_tokens=10, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'i like to murder Turner , but we representative not possess him'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the convenience function:\n",
    "\n",
    "**generate** (*model*, *context*, *max_tokens* = 10, *temperature* = 1.0)\n",
    "\n",
    "> Takes a context sentence *context*, tokenises and vectorises it, and passes it to the specified *model* to generate new text. The new text consists of at most *max_tokens*, but is cut off at the first `<eos>` token. Returns the generated text (including the context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Parameter initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper by Huang, Xiao Shi, et al., titled \"Improving Transformer Optimization Through Better Initialization\" presented at the International Conference on Machine Learning (ICML) in 2020, explores the potential improvements in the optimization of transformer models through enhanced initialization techniques. The authors argue that the traditional methods of initializing transformer models may not be optimal and propose alternative strategies that could potentially enhance model performance, convergence rates, and overall stability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,n_words, num_heads, num_layers, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words,embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        encoderlayer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoderlayer,num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, n_words)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        if x_mask is None:\n",
    "            x_mask = nn.Transformer.generate_square_subsequent_mask(len(x)).to(device)\n",
    "        x = self.transformer_encoder(x,x_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "def uniform(model, inrange = 0.1):\n",
    "    model.embedding.weight.data.uniform_(-inrange, inrange)\n",
    "    model.linear.bias.data.zero_()\n",
    "    model.linear.weight.data.uniform_(-inrange, inrange)\n",
    "    for layer in model.transformer_encoder.layers:\n",
    "        layer.self_attn.out_proj.weight.data.uniform_(-inrange, inrange)\n",
    "        layer.self_attn.out_proj.bias.data.zero_()\n",
    "        layer.linear1.weight.data.uniform_(-inrange, inrange)\n",
    "        layer.linear1.bias.data.zero_()\n",
    "        layer.linear2.weight.data.uniform_(-inrange, inrange)\n",
    "        layer.linear2.bias.data.zero_()\n",
    "\n",
    "def normal(model, mean = 0, sd = 1):\n",
    "    model.embedding.weight.data.normal_(mean=mean , std=sd)\n",
    "    model.linear.bias.data.zero_()\n",
    "    model.linear.weight.data.normal_(mean=mean , std=sd)\n",
    "    for layer in model.transformer_encoder.layers:\n",
    "        layer.self_attn.out_proj.weight.data.normal_(mean=mean , std=sd)\n",
    "        layer.self_attn.out_proj.bias.data.zero_()\n",
    "        layer.linear1.weight.data.normal_(mean=mean , std=sd)\n",
    "        layer.linear1.bias.data.zero_()\n",
    "        layer.linear2.weight.data.normal_(mean=mean , std=sd)\n",
    "        layer.linear2.bias.data.zero_()\n",
    "\n",
    "def xavier(model):\n",
    "    nn.init.xavier_uniform_(model.embedding.weight)\n",
    "    model.linear.bias.data.zero_()\n",
    "    nn.init.xavier_uniform_(model.linear.weight)\n",
    "    for layer in model.transformer_encoder.layers:\n",
    "        nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "        layer.self_attn.out_proj.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "        layer.linear1.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "        layer.linear2.bias.data.zero_()\n",
    "\n",
    "def kaiming(model):\n",
    "    nn.init.kaiming_normal_(model.embedding.weight)\n",
    "    model.linear.bias.data.zero_()\n",
    "    nn.init.kaiming_normal_(model.linear.weight)\n",
    "    for layer in model.transformer_encoder.layers:\n",
    "        nn.init.kaiming_normal_(layer.self_attn.out_proj.weight)\n",
    "        layer.self_attn.out_proj.bias.data.zero_()\n",
    "        nn.init.kaiming_normal_(layer.linear1.weight)\n",
    "        layer.linear1.bias.data.zero_()\n",
    "        nn.init.kaiming_normal_(layer.linear2.weight)\n",
    "        layer.linear2.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:55<00:00, 207.80s/it, perplexity=258, train_loss=4.99, valid_loss=5.55]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:55<00:00, 207.64s/it, perplexity=247, train_loss=4.99, valid_loss=5.51]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:55<00:00, 207.67s/it, perplexity=246, train_loss=4.97, valid_loss=5.51]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:55<00:00, 207.71s/it, perplexity=249, train_loss=4.96, valid_loss=5.52]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:55<00:00, 207.69s/it, perplexity=253, train_loss=5.01, valid_loss=5.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Uniform Initialization: 258.1259765625\n",
      "Perplexity for Normal Initialization: 246.63941955566406\n",
      "Perplexity for Xavier Initialization: 245.9501495361328\n",
      "Perplexity for He Initialization: 249.41943359375\n",
      "Perplexity for Huang et al. Initialization: 252.930419921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history1 = train_rnn(batch_size = 3072//5, lr = 1e-2/5, strategy='Uniform')\n",
    "history2 = train_rnn(batch_size = 3072//5, lr = 1e-2/5, strategy='Normal')\n",
    "history3 = train_rnn(batch_size = 3072//5, lr = 1e-2/5, strategy='Xavier')\n",
    "history4 = train_rnn(batch_size = 3072//5, lr = 1e-2/5, strategy='He')\n",
    "history5 = train_rnn(batch_size = 3072//5, lr = 1e-2/5, strategy='Huang')\n",
    "\n",
    "print('Perplexity for Uniform Initialization:', history1['perplexity'][1].item())\n",
    "print('Perplexity for Normal Initialization:', history2['perplexity'][1].item())\n",
    "print('Perplexity for Xavier Initialization:', history3['perplexity'][1].item())\n",
    "print('Perplexity for He Initialization:', history4['perplexity'][1].item())\n",
    "print('Perplexity for Huang et al. Initialization:', history5['perplexity'][1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose ‚Äúgood‚Äù initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution $\\mathcal{N}(0, 1)$. Test how changing the initialisation affects the perplexity of your language models. Find research articles that propose different initialisation strategies.\n",
    "\n",
    "Write a short (150&nbsp;words) report about your experiments and literature search. Use the following prompts:\n",
    "\n",
    "* What different initialisation did you try? What results did you get?\n",
    "* How do your results compare to what was suggested by the research articles?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: In our experiments, we have tried four different initialization strategies for weights in embedding layer, lstm and final layer of the RNN model. In specific, we initialised weights using Uniform distribution in range (-1,1), Normal distribution with mean=0, std=1, Xavier Initialization and He initialization. The best perplexity result was achieved by ______ with the lowest value of ____. The difference in the perplexity value of _____ and the random initialization (in problem 2) is ____ that justifies the importance of weight initialization in the neural network training.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
